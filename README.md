# CodeTransCoT: Enhancing quality of Code Translation with Structured Chain-of-Thought Prompting

- We introduce CodeTransCoT, a novel Chain-of-Thought based technique designed to efficiently and accurately translate code from one language to another while preserving structural similarity.
- We propose a Quality metric to measure structural similarity and verify if the generated program matches the intended output. This involves performing static analysis of the code and extracting call graph

The approach and code can be found in `CodeTransCoT` folder. `QualityEval` contains the code to evaluate quality measure of the generated programs using `CodeTransCoT`.

### Requirements
Execute the following to install all requirements:
```
pip3 install -r requirements.txt
```
<br>
Structure should be followed before running the code-
```
./data/dataset
./data/outputs
./data/huggingface

./models/model_folders
```
### Dataset

The dataset used in this study is available on [HuggingFace](https://huggingface.co/iidai). 

#### Translation

The first step is to use the model for generating raw translations. A sample translation command is provided below:

```
python3 ./CodeTrans/translate/translate.py --model=granite-8b-code-instruct --dataset=avatar --source_lang=Python --target_lang=Java --prompt_type=codetranscot --temperature=0.2 --n_samples=10 --batch_size=10 --max_length=4090 --ngpus=2
```

#### Sanitization

The raw translations generated by LLMs contain extra template-related tokens and natural language. A sample sanitization command is provided below:

```
python3 ./CodeTrans/clean_generations.py --source_lang Python --target_lang Java --model granite-8b-code-instruct --approach codetranscot --dataset avatar
```

#### Evaluation

The final step is to evaluate the correctness of sanitized translations. A sample evaluation command is given below:

```
python3 ./CodeTrans/compile_avatar.py --source_lang Python --target_lang Java --model granite-8b-code-instruct --approach codetranscot --dataset avatar
```

### Declaration
Team work!
